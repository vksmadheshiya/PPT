{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Linear Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the purpose of the General Linear Model (GLM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** Generalized Linear Models (GLMs) are a class of regression models that can be used to model a wide range of relationships between a response variable and one or more predictor variables. Unlike traditional linear regression models, which assume a linear relationship between the response and predictor variables, GLMs allow for more flexible, non-linear relationships by using a different underlying statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the key assumptions of the General Linear Model?\n",
    "\n",
    "**ANS:** Key assumptions of the General Linear Model are :\n",
    "First, the general linear model assumes that the relationships between the outcome and any continuous predictors are linear; in Figure 1, the straight regression line adequately captures the trend in the data. (You can add non-linear transformations of the predictors, too, but then the model will assume that the transformed predictors are linearly related to the outcome. I’m using the term ‘linearity assumption’ to mean ‘the model captures the trend in the data’.)\n",
    "\n",
    "Second, the general linear model assumes that the residuals were all sampled from the same distribution. The three red curves in Figure 1 show the assumed distribution of the residuals at three predictor values; the ‘constant variance’ assumption entails that these distributions all have the same width as they do in Figure 1.\n",
    "\n",
    "Third, the general linear model assumes that the residuals were all sampled from normal distributions. Strictly speaking, t- and F-statistics – and the p-values derived from them – depend on this assumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How do you interpret the coefficients in a GLM?\n",
    "\n",
    "**ANS:** We can interpret your coefficients in a similar way as the Poisson regression. When you increase x by 1, the mean of your underlying count (which you have turned into presence/absence) is multiplied by exp(β1) e x p ( β 1 ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the difference between a univariate and multivariate GLM?\n",
    "\n",
    "**ANS:**\n",
    "The difference between a univariate and multivariate Generalized Linear Model (GLM) lies in the number of dependent variables (responses) being modeled.\n",
    "\n",
    "1. **Univariate GLM:** In a univariate GLM, there is a single dependent variable or response variable being modeled. The model estimates the relationship between this single response variable and one or more independent variables (predictors). The focus is on understanding and modeling the relationship between the response variable and the predictors.\n",
    "\n",
    "2. **Multivariate GLM:** In a multivariate GLM, there are multiple dependent variables or response variables being modeled simultaneously. The model estimates the relationships between the multiple response variables and the predictors. The focus is on understanding and modeling the relationships between the response variables themselves, as well as their relationships with the predictors.\n",
    "\n",
    "Key differences between univariate and multivariate GLMs include:\n",
    "\n",
    "- **Number of Response Variables:** Univariate GLM deals with one response variable, whereas multivariate GLM deals with two or more response variables.\n",
    "\n",
    "- **Modeling Approach:** Univariate GLM focuses on modeling the relationship between a single response variable and predictors. Multivariate GLM considers the relationships between multiple response variables simultaneously, accounting for their potential interdependencies.\n",
    "\n",
    "- **Parameter Estimation:** In univariate GLM, separate parameter estimates are obtained for each predictor in relation to the single response variable. In multivariate GLM, parameter estimates are obtained for predictors in relation to each response variable, as well as for the relationships among the response variables themselves.\n",
    "\n",
    "- **Model Complexity:** Multivariate GLM models tend to be more complex than univariate GLM models because they involve analyzing and capturing relationships between multiple response variables.\n",
    "\n",
    "Univariate GLM is often used when analyzing the relationship between a single outcome variable and multiple predictors. On the other hand, multivariate GLM is employed when investigating the relationships between multiple outcome variables, such as in multivariate regression or multivariate analysis of variance (MANOVA) scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain the concept of interaction effects in a GLM.\n",
    "\n",
    "**ANS:**  In general, the existence of an interaction means that the effect of one variable depends on the value of the other variable with which it interacts. If there isn't an interaction, then the value of the other variable doesn't matter.\n",
    "Interaction effects in GLMs describing probabilities and counts are not equal to product terms between predictor variables. Instead, interactions may be functions of the predictors of a model, requiring nontraditional approaches for interpreting these effects accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How do you handle categorical predictors in a GLM?\n",
    "\n",
    "**ANS:** When your dependent variable is binary (1 vs. 0, \"dead\" vs. \"alive\"), the you might use logistic regression which is a glm with a binomial error distribution and a logit link function. When your dependent variable is ordinal (e.g. \"bad\"> \"good\" > \"best\"), you can use ordinal logistic regression. For a nominal (e.g. transportation: \"walk\", \"car\", \"bicycle\") dependent variable, you can use multinomial logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is the purpose of the design matrix in a GLM?\n",
    "\n",
    "**ANS:** The design matrix is used in certain statistical models, e.g., the general linear model. It can contain indicator variables (ones and zeros) that indicate group membership in an ANOVA, or it can contain values of continuous variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How do you test the significance of predictors in a GLM?\n",
    "\n",
    "**ANS :**  likelihood ratio test, F-test and ANNOVA are used to tests are used to test the significance of predictors in a GLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
    "\n",
    "**ANS :** Three different methodologies for splitting variation exist: Type I, Type II and Type III Sums of Squares. They do not give the same result in case of unbalanced data.\n",
    "\n",
    "Type I Sums of Squares, or also called Sequential Sums of Squares, assign variation to the different variables in a sequential order.\n",
    "\n",
    "The Type II Sums of Squares take a different approach in two ways.\n",
    "\n",
    "First of all, the variation assigned to independent variable A is accounting for B and the other way around the variation assigned to B is accounting for A.\n",
    "Secondly, the Type II Sums of Squares do not take an interaction effect.\n",
    "\n",
    "\n",
    "The Type III Sums of Squares are also called partial sums of squares again another way of computing Sums of Squares:\n",
    "\n",
    "Like Type II, the Type III Sums of Squares are not sequential, so the order of specification does not matter.\n",
    "Unlike Type II, the Type III Sums of Squares do specify an interaction effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Explain the concept of deviance in a GLM.\n",
    "\n",
    "**ANS :** Deviance is a goodness-of-fit metric for statistical models, particularly used for GLMs. It is defined as the difference between the Saturated and Proposed Models and can be thought as how much variation in the data does our Proposed Model account for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What is regression analysis and what is its purpose?\n",
    "\n",
    "**ANS :** Regression analysis is a statistical method that shows the relationship between two or more variables. Usually expressed in a graph, the method tests the relationship between a dependent variable against independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "12. What is the difference between simple linear regression and multiple linear regression?\n",
    "\n",
    "**ANS : ** Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. How do you interpret the R-squared value in regression?\n",
    "\n",
    "**ANS:** R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. What is the difference between correlation and regression?\n",
    "\n",
    "**ANS:** The key difference between correlation and regression is that correlation measures the degree of a relationship between two independent variables (x and y). In contrast, regression is how one variable affects another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. What is the difference between the coefficients and the intercept in regression?\n",
    "\n",
    "**ANS:** The simple linear regression model is essentially a linear equation of the form y = c + b*x; where y is the dependent variable (outcome), x is the independent variable (predictor), b is the slope of the line; also known as regression coefficient and c is the intercept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. How do you handle outliers in regression analysis?\n",
    "\n",
    "**ANS:** There are many possible approaches to dealing with outliers: \n",
    "1. removing them from the observations, \n",
    "2. treating them (for example, capping the extreme observations at a reasonable value), \n",
    "3. using algorithms that are well-suited for dealing with such values on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. What is the difference between ridge regression and ordinary least squares regression?\n",
    "\n",
    "**ANS:** In summary, when there is a difference in variance between predictor variables, OLS tends to give higher variance for coefficients corresponding to predictors with higher variance, while Ridge Regression reduces the variance differences between coefficients by shrinking them towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. What is heteroscedasticity in regression and how does it affect the model?\n",
    "\n",
    "**ANS:** Heteroskedastic refers to a condition in which the variance of the residual term, or error term, in a regression model varies widely. Homoskedastic refers to a condition in which the variance of the error term in a regression model is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. How do you handle multicollinearity in regression analysis?\n",
    "\n",
    "**ANS:** The idea is to reduce the multicollinearity by regularization by reducing the coefficients of the feature that are multicollinear. By increasing the alpha value for the L1 regularizer, we introduce some small bias in the estimator that breaks the correlation and reduces the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. What is polynomial regression and when is it used?\n",
    "\n",
    "**ANS:** A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. What is a loss function and what is its purpose in machine learning?\n",
    "\n",
    "**ANS:** At its core, a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value). We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. What is the difference between a convex and non-convex loss function?\n",
    "\n",
    "**ANS:** A convex function is one in which a line drawn between any two points on the graph lies on the graph or above it. There is only one requirement. A non-convex function is one in which a line drawn between any two points on the graph may cross additional points. It was described as “wavy.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. What is mean squared error (MSE) and how is it calculated?\n",
    "\n",
    "**ANS:** The Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss. Mean square error is calculated by taking the average, specifically the mean, of errors squared from data as it relates to a function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. What is mean absolute error (MAE) and how is it calculated?\n",
    "\n",
    "\n",
    "**ANS:** MAE is calculated as the sum of absolute errors divided by the sample size: It is thus an arithmetic average of the absolute errors , where is the prediction and. the true value. Alternative formulations may include relative frequencies as weight factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. What is log loss (cross-entropy loss) and how is it calculated?\n",
    "\n",
    "**ANS:** Log Loss (Binary Cross-Entropy Loss): A loss function that represents how much the predicted probabilities deviate from the true ones. It is used in binary cases. Cross-Entropy Loss: A generalized form of the log loss, which is used for multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. How do you choose the appropriate loss function for a given problem?\n",
    "\n",
    "**ANS:** There's no one-size-fits-all loss function to algorithms in machine learning. There are various factors involved in choosing a loss function for specific problem such as type of machine learning algorithm chosen, ease of calculating the derivatives and to some degree the percentage of outliers in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. Explain the concept of regularization in the context of loss functions.\n",
    "\n",
    "**ANS:**Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. What is Huber loss and how does it handle outliers?\n",
    "\n",
    "**ANS:** In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. What is quantile loss and when is it used?\n",
    "\n",
    "**ANS:**As the name suggests, the quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. What is the difference between squared loss and absolute loss?\n",
    "\n",
    "**ANS:** For square loss, you will choose the estimated mean of y0, as the true mean minimizes square loss on average (where the average is taken across random samples of y0 subject to x=x0). For absolute loss, you will choose the estimated median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer (GD):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. What is an optimizer and what is its purpose in machine learning?\n",
    "\n",
    "**ANS:** An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss. But it is a daunting task to choose the appropriate weights for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. What is Gradient Descent (GD) and how does it work?\n",
    "\n",
    "**ANS:** Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33. What are the different variations of Gradient Descent?\n",
    "\n",
    "**ANS:** There are three types of gradient descent learning algorithms: batch gradient descent, stochastic gradient descent and mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34. What is the learning rate in GD and how do you choose an appropriate value?\n",
    "\n",
    "**ANS :** In order for Gradient Descent to work, we must set the learning rate to an appropriate value. This parameter determines how fast or slow we will move towards the optimal weights. If the learning rate is very large we will skip the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. How does GD handle local optima in optimization problems?\n",
    "\n",
    "**ANS:** The cost function may consist of many minimum points. The gradient may settle on any one of the minima, which depends on the initial point (i.e initial parameters(theta)) and the learning rate. Therefore, the optimization may converge to different points with different starting points and learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
    "\n",
    "**ANS:** Stochastic Gradient Descent is a drastic simplification of GD which overcomes some of its difficulties. Each iteration of SGD computes the gradient on the basis of one randomly chosen partition of the dataset which was shuffled, instead of using the whole part of the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37. Explain the concept of batch size in GD and its impact on training.\n",
    "\n",
    "**ANS:** Batch size defines the number of samples we use in one epoch to train a neural network. There are three types of gradient descent in respect to the batch size: Batch gradient descent – uses all samples from the training set in one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38. What is the role of momentum in optimization algorithms?\n",
    "\n",
    "**ANS:** Momentum is a strategy for accelerating the convergence of the optimization process by including a momentum element in the update rule. This momentum factor assists the optimizer in continuing to go in the same direction even if the gradient changes direction or becomes zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
    "\n",
    "**ANS:** Mini Batch Gradient Descent, Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is large. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40. How does the learning rate affect the convergence of GD?\n",
    "\n",
    "**ANS:** In order for Gradient Descent to work, we must set the learning rate to an appropriate value. This parameter determines how fast or slow we will move towards the optimal weights. If the learning rate is very large we will skip the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41. What is regularization and why is it used in machine learning?\n",
    "\n",
    "\n",
    "**ANS:** Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42. What is the difference between L1 and L2 regularization?\n",
    "\n",
    "**ANS:**L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43. Explain the concept of ridge regression and its role in regularization.\n",
    "\n",
    "**ANS:** Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions. Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
    "\n",
    "**ANS:** In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45. How does regularization help prevent overfitting in machine learning models?\n",
    "\n",
    "**ANS:** In short, Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46. What is early stopping and how does it relate to regularization?\n",
    "\n",
    "**ANS:** The model at the time that training is stopped is then used and is known to have good generalization performance. This procedure is called “early stopping” and is perhaps one of the oldest and most widely used forms of neural network regularization. This strategy is known as early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47. Explain the concept of dropout regularization in neural networks.\n",
    "\n",
    "**ANS:** Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term \"dropout\" refers to dropping out units (both hidden and visible) in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48. How do you choose the regularization parameter in a model?\n",
    "\n",
    "**ANS:** How do we choose the regularization parameter? as follows: on the training set, we estimate several different Ridge regressions, with different values of the regularization parameter; on the validation set, we choose the best model (the regularization parameter which gives the lowest MSE on the validation set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49. What is the difference between feature selection and regularization?\n",
    "\n",
    "**ANS:** Feature selection, also known as feature subset selection, variable selection, or attribute selection. This approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model inference. Regularization, where we are constraining the solution space while doing optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50. What is the trade-off between bias and variance in regularized models?\n",
    "\n",
    "**ANS:** Bias Variance Tradeoff\n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51. What is Support Vector Machines (SVM) and how does it work?\n",
    "\n",
    "**ANS:** An SVM builds a learning model that assigns new examples to one group or another. By these functions, SVMs are called a non-probabilistic, binary linear classifier. In probabilistic classification settings, SVMs can use methods such as Platt Scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52. How does the kernel trick work in SVM?\n",
    "\n",
    "**ANS:** The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations x (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations ϕ(x) and representing the data by these transformed coordinates in the higher dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "53. What are support vectors in SVM and why are they important?\n",
    "\n",
    "**ANS:** Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54. Explain the concept of the margin in SVM and its impact on model performance.\n",
    "\n",
    "**ANS:** Margin: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "55. How do you handle unbalanced datasets in SVM?\n",
    "\n",
    "**ANS:** We can use the make_classification() function to define a synthetic imbalanced two-class classification dataset. We will generate 10,000 examples with an approximate 1:100 minority to majority class ratio. Once generated, we can summarize the class distribution to confirm that the dataset was created as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "56. What is the difference between linear SVM and non-linear SVM?\n",
    "\n",
    "**ANS:** Linear SVM: When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data. Non-linear SVM: When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
    "\n",
    "**ANS:** C parameter adds a penalty for each misclassified data point. If c is small, the penalty for misclassified points is low so a decision boundary with a large margin is chosen at the expense of a greater number of misclassifications ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "58. Explain the concept of slack variables in SVM.\n",
    "\n",
    "Slack variables are introduced to allow certain constraints to be violated. That is, certain train- ing points will be allowed to be within the margin. We want the number of points within the margin to be as small as possible, and of course we want their penetration of the margin to be as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "59. What is the difference between hard margin and soft margin in SVM?\n",
    "\n",
    "**ANS:** The difference between a hard margin and a soft margin in SVMs lies in the separability of the data. If our data is linearly separable, we go for a hard margin. However, if this is not the case, it won't be feasible to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60. How do you interpret the coefficients in an SVM model?\n",
    "\n",
    "**ANS:** Let's say the svm would find only one feature useful for separating the data, then the hyperplane would be orthogonal to that axis. So, you could say that the absolute size of the coefficient relative to the other ones gives an indication of how important the feature was for the separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "61. What is a decision tree and how does it work?\n",
    "\n",
    "**ANS:** A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "62. How do you make splits in a decision tree?\n",
    "\n",
    "**ANS:** Steps to split a decision tree using Information Gain:\n",
    "For each split, individually calculate the variance of each child node\n",
    "Calculate the variance of each split as the weighted average variance of child nodes\n",
    "Select the split with the lowest variance\n",
    "Perform steps 1-3 until completely homogeneous nodes are achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
    "\n",
    "**ANS:** The Gini impurity measure is one of the methods used in decision tree algorithms to decide the optimal split from a root node, and subsequent splits. To put it into context, a decision tree is trying to create sequential questions such that it partitions the data into smaller groups.\n",
    "\n",
    "The Entropy and Information Gain method focuses on purity and impurity in a node. The Gini Index or Impurity measures the probability for a random instance being misclassified when chosen randomly. The lower the Gini Index, the better the lower the likelihood of misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64. Explain the concept of information gain in decision trees.\n",
    "\n",
    "**ANS:** Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65. How do you handle missing values in decision trees?\n",
    "\n",
    "**ANS:** Surrogate splitting rules enable you to use the values of other input variables to perform a split for observations with missing values. Important Note : Tree Surrogate splitting rule method can impute missing values for both numeric and categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66. What is pruning in decision trees and why is it important?\n",
    "\n",
    "**ANS:** Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "67. What is the difference between a classification tree and a regression tree?\n",
    "\n",
    "**ANS:** Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68. How do you interpret the decision boundaries in a decision tree?\n",
    "\n",
    "**ANS:** A Decision tree splits the data based on a feature value and this value would remain constant throughout for one decision boundary e.g., x=2 or y=3 where x and y are two different features. Whereas in a linear classifier, a decision boundary could be for instance: y=mx+c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "69. What is the role of feature importance in decision trees?\n",
    "\n",
    "**ANS:** A decision tree is explainable machine learning algorithm all by itself. Beyond its transparency, feature importance is a common way to explain built models as well. Coefficients of linear regression equation give a opinion about feature importance but that would fail for non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70. What are ensemble techniques and how are they related to decision trees?\n",
    "\n",
    "**ANS:** Ensemble methods, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "71. What are ensemble techniques in machine learning?\n",
    "\n",
    "**ANS:** In this ensemble technique, machine learning professionals use a number of models for making predictions about each data point. The predictions made by different models are taken as separate votes. Subsequently, the prediction made by most models is treated as the ultimate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72. What is bagging and how is it used in ensemble learning?\n",
    "\n",
    "**ANS:** Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. In bagging, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73. Explain the concept of bootstrapping in bagging.\n",
    "\n",
    "**ANS:** Bootstrapping is a sampling method, where a sample is chosen out of a set, using the replacement method. The learning algorithm is then run on the samples selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74. What is boosting and how does it work?\n",
    "\n",
    "**ANS:** Boosting creates an ensemble model by combining several weak decision trees sequentially. It assigns weights to the output of individual trees. Then it gives incorrect classifications from the first decision tree a higher weight and input to the next tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75. What is the difference between AdaBoost and Gradient Boosting?\n",
    "\n",
    "**ANS:** AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem. This makes Gradient Boosting more flexible than AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76. What is the purpose of random forests in ensemble learning?\n",
    "\n",
    "**ANS:** Random forest algorithm is an ensemble learning technique combining numerous classifiers to enhance a model's performance. Random Forest is a supervised machine-learning algorithm made up of decision trees. Random Forest is used for both classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77. How do random forests handle feature importance?\n",
    "\n",
    "**ANS:** The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78. What is stacking in ensemble learning and how does it work?\n",
    "\n",
    "**ANS:** Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. Stacking enables us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "79. What are the advantages and disadvantages of ensemble techniques?\n",
    "\n",
    "**ANS:** Ensemble methods offer several advantages over single models, such as improved accuracy and performance, especially for complex and noisy problems. They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance, and by using different subsets and features of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80. How do you choose the optimal number of models in an ensemble?\n",
    "\n",
    "**ANS:**\n",
    "Step 1 : Find the KS of individual models.\n",
    "Step 2: Index all the models for easy access.\n",
    "Step 3: Choose the first two models as the initial selection and set a correlation limit.\n",
    "Step 4: Iteratively choose all the models which are not highly correlated with any of the any chosen model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
